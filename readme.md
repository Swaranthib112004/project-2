your\_project\_root/
├── MultipleFiles/
│   ├── app\_streamlit.py
│   ├── concepts\_per\_item.json
│   ├── convert\_to\_neuralcdm\_json.py # Keep if other tools use it, but not used by new NCDM
│   ├── item\_difficulty.csv           # Generated by predict.py
│   ├── items.csv
│   ├── llm\_concepts.py               # MODIFIED
│   ├── metrics.json                  # Generated by predict.py
│   ├── q\_matrix\_optimized.csv
│   ├── q\_matrix\_raw.csv
│   ├── questions\_bloom.csv
│   ├── responses.csv
│   ├── responses.json                # Generated by convert\_to\_neuralcdm\_json.py (if used)
│   ├── run\_pipeline.py               # MODIFIED
│   ├── sbert\_optimize\_q.py
│   ├── student\_mastery.csv           # Generated by predict.py
│   └── training\_metrics.csv          # Generated by train.py
├── data/
│   ├── config.txt                    # NEW (manual creation based on data)
│   ├── log\_data.json                 # NEW (generated by create\_log\_data.py)
│   ├── test\_set.json                 # Generated by dividedata.py
│   ├── train\_set.json                # Generated by dividedata.py
│   ├── train\_slice.json              # Generated by dividedata.py
│   └── val\_set.json                  # Generated by dividedata.py
├── model/                            # NEW directory for NCDM model snapshots
│   └── model\_epochX                  # Generated by train.py
├── ncdm\_src/                         # NEW directory for NCDM source code
│   ├── data\_loader.py                # NEW
│   ├── dividedata.py                 # NEW
│   ├── model.py                      # NEW
│   ├── predict.py                    # NEW
│   └── train.py                      # NEW
├── result/                           # NEW directory for NCDM text results
│   ├── model\_test.txt                # Generated by predict.py
│   └── model\_val.txt                 # Generated by train.py
├── create\_log\_data.py                # NEW (helper script)
└── requirements.txt                  # MODIFIED





Step 1: Prepare your environment



&nbsp;       python -m venv .venv

\# Windows

&nbsp;      .venv\\Scripts\\activate

\# macOS/Linux

&nbsp;      source .venv/bin/activate



&nbsp;      pip install -r requirements.txt



&nbsp;      ollama pull llama3.1:8b



Step 2: Run LLM concept extraction

&nbsp;

&nbsp;      $env:LLM_MODEL="llama3.1:8b"



&nbsp;      python llm\_concepts.py     (or)      python MultipleFiles/llm_concepts.py data/items.csv



Step 3: Create log\_data.json for NCDM


 
&nbsp;      python MultipleFiles/create\_log\_data.py      (or)    python MultipleFiles/create_log_data.py data/responses.csv data/log_data.json



Step 4: Create data/config.txt



&nbsp;      student\_n,exer\_n,knowledge\_n

&nbsp;      12,10,17



Step 5: Divide data into train/val/test sets



&nbsp;      python ncdm_src/dividedata.py



Step 6: Build and optimize Q-matrix



&nbsp;      python MultipleFiles/sbert\_optimize\_q.py   (or)      python MultipleFiles/sbert_optimize_q.py data/items.csv



Step 7: Train the NCDM model



&nbsp;      python -m ncdm_src/train.py cpu 10



Step 8: Evaluate the model and extract mastery/difficulty



&nbsp;      python -m ncdm_src.predict 10



Step 9: Run the Streamlit dashboard



&nbsp;      streamlit run MultipleFiles/app_streamlit.py



Optional: Run the full pipeline script



&nbsp;      python -m MultipleFiles.run_pipeline   



&nbsp;   Note: You still need to create data/config.txt manually after step 3 before continuing.

  
